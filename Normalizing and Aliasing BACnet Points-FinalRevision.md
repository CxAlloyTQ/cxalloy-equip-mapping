# Normalizing and Aliasing BACnet Points for SkySpark–CxAlloy Integration

## Introduction

Integrating a commissioning platform like CxAlloy with a SkySpark server (connected via BACnet to building automation systems) requires bridging a semantic gap: the building's live data points often have cryptic names, whereas the commissioning process needs clear, standardized labels. BACnet device points typically come with terse identifiers or acronyms devised by controls technicians, making it challenging to directly use them for automated functional testing or data analysis. The core challenge is to normalize these raw point names – assigning each a human-readable alias and a consistent set of metadata tags – so that SkySpark and CxAlloy share a common understanding of each equipment's data. As one industry expert lamented, "we are stuck with thousands of buildings with non-standard point names" in the field, underscoring the importance of this normalization step.

In this report, I provide a technical guide to achieve that goal. I'll start with an overview of how BACnet represents point data and how SkySpark ingests it. Then I'll discuss methods to parse and interpret cryptic BACnet point labels. Using semantic tagging frameworks (like Project Haystack), I outline strategies to create meaningful aliases and standardized tags for points. I also explore techniques such as pattern recognition and even machine learning to automate the mapping process, and how to leverage additional metadata (equipment type, model info, vendor descriptions) to improve accuracy. SkySpark-specific tools (connectors, Axon scripting, tagging features) are highlighted as enablers for this workflow. Finally, I present a step-by-step workflow to normalize and alias BACnet points, facilitating seamless mapping to CxAlloy's equipment definitions and enabling automated testing and trending.

## Understanding BACnet Point Data and SkySpark Ingestion

**BACnet Overview:** BACnet (Building Automation and Control Networks) is a standard protocol for building automation that defines how devices and points (sensors, actuators, etc.) communicate. BACnet organizes data into a hierarchy of devices and their objects (points). Each point on a BACnet device is represented as an object with properties like an Object Name, Object Type (analog input, binary output, etc.), a Present Value, Units, and optional Description among others. The BACnet standard provides a common language for describing equipment and point functionality, which is why it's widely used for integrating multi-vendor systems. In practice, however, the Object Name set by the device or integrator can be an abbreviated, cryptic label (e.g. Zn_T for zone temperature). BACnet supports standardized services for data sharing (reading/writing point values), trending, alarming, and device discovery.

**SkySpark Data Ingestion:** SkySpark connects to BACnet systems via its BACnet connector (using BACnet/IP for IP networks). A typical workflow is to perform a discovery: SkySpark will query the network to find BACnet devices and enumerate their points. Just as some tools allow browsing a device's points tree (for example, FIN Stack's DB Builder can discover devices and points via drag-and-drop), SkySpark's connector provides a list of discovered BACnet points. An integrator can then select or bulk-import these points into SkySpark's database. When a point is imported, SkySpark creates a record in its Folio database representing that point and sets up the link to the BACnet source. The record will carry basic information from the BACnet device: typically the raw point name (often mapped into a navName or similar tag), the data type (Number vs Boolean), perhaps the unit (if BACnet reported a standard engineering units code), and the necessary connector references (like BACnet device ID, object ID) so SkySpark can read the value. From that point on, SkySpark can poll the BACnet points or subscribe to COV (Change-of-Value) notifications to get live data.

**Trio Files:** In this scenario, each piece of equipment's points have been exported as a trio-formatted text file. Trio (.trio) is a Project Haystack file format (Tag Record IO) designed for human-readable data exchange. It lists records (like point definitions) with one tag per line, separated by --- between records. In essence, a trio file here likely contains all point records for an equipment, including their raw BACnet names and any initial tags. Using trio files is convenient because you can edit them in a text editor to add tags or change labels, then import them back into SkySpark. It's a format meant for "hand coding" Haystack data and is fully supported by SkySpark for data import/export.

**SkySpark's Data Model:** SkySpark uses the Project Haystack semantic data model under the hood. This means data is stored as a collection of entity records with tags (key–value pairs or markers) describing each entity. For example, a point imported from BACnet will at minimum have the tag point (to mark it as a point), and typically an id (unique identifier), siteRef (linking to the site or project), equipRef (linking to an equipment record), and so on. Initially, the point might not have many semantic tags beyond those needed for connectivity (and whatever minimal metadata came from BACnet like units). My task is to enrich these records with additional tags and a cleaned-up name (alias) so that both humans and software can easily identify them. SkySpark's Folio database allows me to add any number of tags to each point record, and these tags are how SkySpark (and any Haystack client, like analytics or external APIs) understands what the point represents. In short, SkySpark will serve as the normalization layer: it ingests raw BACnet data and gives me a place to attach standardized labels and tags.

## Parsing and Interpreting Cryptic BACnet Point Labels

The first step to normalization is understanding the meaning of each raw point name. BACnet point labels (object names) are often limited in length and filled with acronyms or codes. They might follow a naming convention that made sense to the installer or controls programmer, but not immediately obvious to others. Here's how to systematically parse them:

**Use All Available Clues from BACnet:** Don't rely solely on the raw name string. BACnet provides other metadata that can aid interpretation:

• **Point Name (Object Name):** This is the primary identifier. Start by examining it for recognizable patterns or abbreviations. Break it into components if it's compound. For example, AHU1_SAT could be split into AHU1 and SAT. Perhaps AHU1 indicates the equipment (Air Handling Unit 1) and SAT is a known acronym (Supply Air Temperature). Look at capitalization, underscores, or dashes as separators. Common patterns include `<EquipName>_<PointName>` or hierarchical names like Floor1.AHU1.SAT.

• **Units:** The engineering units property from BACnet is extremely useful. If a point's units are degrees Fahrenheit (°F) or Celsius, it's almost certainly a temperature reading. Units of % often signify a percentage command or sensor (like valve position or relative humidity). Units of kW or kWh indicate power or energy. Units can distinguish between similar-sounding points (e.g., a point named "Temp_SP" with units °C is clearly a temperature setpoint). Always cross-reference the unit; it can confirm or refute your guess from the name.

• **Object Type / Data Type:** BACnet object types tell you if it's analog vs binary, input vs output, etc. A Binary Output called "Fan" likely means a command to a fan (on/off), whereas a Binary Input "Fan Proof" would be a status indicator. Analog Inputs are usually sensor readings, Analog Outputs or Analog Values might be setpoints or commands. The data type (Boolean vs numeric) also helps: if a point is Boolean and named "Status" or "Enable", it's a binary on/off indicator. If it's numeric and named "SP" (setpoint), it's an analog value.

• **Description:** Many BACnet devices have an optional Description field for each point. Sometimes integrators put a human-friendly description here (e.g., an Object Name might be RmTemp but Description is "Room Temperature Sensor"). If your data source includes descriptions, use them as a Rosetta stone. The description can directly be used as the alias or at least provide words to match against (e.g., if description contains "temperature", you know it's a temp sensor).

• **Present Value and Trends:** If you have access to live data or historical samples, the behavior of the value is a clue. A point that only ever shows 0 or 1 is likely a binary status/command. A point that hovers around room temperature (~70°F) is probably a space temp reading. One that stays at a fixed value until someone changes it is likely a setpoint. This kind of statistical inference – looking at min/max, variance, frequency of change – can separate sensors from setpoints. It's a more advanced clue, but worth mentioning for tricky cases.

**Recognize Common Acronyms and Naming Patterns:** Over the years, certain abbreviations are very commonly used in BAS point names. Knowing these conventions can rapidly speed up parsing:

For example, Temp or T usually means temperature. SP means setpoint. Cmd (or just the absence of "sensor" or "status") might imply a command. Pos or Pct might indicate a percent output (valve or damper position). Fan St or Fan Status vs Fan Cmd differentiate a fan feedback versus a command. Many systems use two-letter prefixes/suffixes: SF for Supply Fan, EF for Exhaust Fan, HW for Hot Water, CHW for Chilled Water, AHU for Air Handling Unit, VAV for Variable Air Volume box, etc. Identifying these equipment or system codes in the point name can tell you the context.

Consider this partial list of common acronyms and their interpretations:

| Acronym | Likely Meaning | Interpretation and Tags |
|---------|----------------|------------------------|
| SAT | Supply Air Temperature | Discharge air temp sensor (on supply duct). Tags: temp, sensor, air, discharge (often on an AHU) |
| RAT | Return Air Temperature | Return air temperature sensor. Tags: temp, sensor, air, return |
| ZN-T (ZNT) | Zone Temperature | Zone (room) temperature sensor. Tags: temp, sensor, zone |
| ZN-SP | Zone Temperature Setpoint | Zone temp setpoint. Tags: temp, sp, zone (and possibly sensor if it's the current effective setpoint) |
| SF | Supply Fan (Status or Command) | Could be supply fan status sensor (if read-only) or command (if writable). Context needed. Tags: fan, sensor or fan, cmd |
| EF | Exhaust Fan (Status/Command) | Similar to SF, for exhaust fan. |
| HW VL | Hot Water Valve | Hot water valve position command (if analog 0-100%) or status. Tags: valve, hotWater, cmd (if controllable) |
| DX | Direct Expansion (cooling) | Often prefix for cooling stages or statuses in unitary equipment. e.g., DX1 St might be first stage compressor status. |
| OA | Outside Air | Used for outside air damper or temperature. e.g., OA Damper Pos (outdoor air damper position) or OA Temp (outdoor air temp sensor). |
| Cmd or CM | Command | Suffix indicating this point writes a command to a device (as opposed to reading a sensor). Often binary. Tags: cmd (and whatever it commands, e.g. fan). |
| Status or STS | Status/State | Suffix for a feedback point (usually binary) indicating the state of a device. Tags: sensor (and e.g. fan if fan status). |
| Pos | Position | Suffix for a position signal (usually %). Common for valves, dampers. Tags: sensor if it's a feedback, or cmd if it's a control output to a modulating actuator. |
| Flow or Fl | Flow | Could refer to airflow in CFM (for VAV) or water flow in GPM. Use units to tell. Tags: flow, sensor (and medium: air or water). |
| Occ | Occupancy | Occupancy status or mode. Could be a binary occupied/unoccupied status or a schedule indicator. Tags: occupancy, sensor (or possibly cmd if controlling mode). |

**Note:** These are general examples – actual naming conventions vary widely by vendor and project. Always confirm your interpretations with context. For instance, "EF" might mean Exhaust Fan in one building, but a different acronym in another context. Also, some acronyms like SP can be ambiguous (setpoint vs static pressure, depending on context and units). Therefore, use multiple clues together; don't rely on the acronym alone.

**Equipment Context Matters:** As mentioned, knowing the equipment type guides your expectations. If you know a file is for an AHU (air handler), terms like SAT, RAT, SF, "Cool Stg" (cooling stage), "HW Valve" make sense. If it's a VAV box, you expect zone temp, damper position, perhaps reheat valve or supply flow. If it's a Chiller, you look for things like evaporator entering/leaving temperature, compressor status, etc. Use the equipment name or model to inform which acronyms are likely. Many acronyms are only unambiguous within a certain equipment type (e.g., "CHW" for chilled water will appear with chillers or coils, not with VAVs).

**Parsing Strategy:** For each point name:

• **Normalize the string:** Replace delimiters with spaces (e.g., underscores _ or dashes - to spaces), split CamelCase into words, etc. This yields tokens you can analyze.

• **Match tokens against known dictionary:** Have a dictionary of common acronyms (like the above table or an even more project-specific list) and see if any token is in the dictionary. If yes, assign preliminary meaning (e.g., token "Temp" -> this is a temperature-related point, likely a sensor unless another token says SP).

• **Look at units and type:** Refine the interpretation using units/type. If your dictionary match was "SP" meaning setpoint, but the point is a Binary Output, maybe "SP" in this case was something else (or the point type is misconfigured). Generally, an Analog Value with units of temperature and a token "SP" is indeed a temperature setpoint. A Binary with "Status" is indeed a status indicator, etc.

• **Combine tokens for full meaning:** Often the point's full meaning comes from a combination. For example, "Pump1 Status" has tokens [Pump1, Status] -> equipment Pump1, point is a status sensor. Or "ZoneTemp_SP" -> zone temperature setpoint. Order and combination matter; you might have logic like: "if both 'Temp' and 'SP' tokens are present, it's a temp setpoint".

• **Consult external references if needed:** If something is unclear, a quick search in the equipment's manual or even online might help. Some vendors use unique abbreviations; you might find a document or forum post decoding those.

• **Iterate and document:** As you interpret points, keep notes of what each acronym meant in this project. This becomes your project-specific naming convention documentation. It's helpful not only for you but for handing off to others or for future projects (as starting dictionary).

By systematically applying these steps, you can derive an initial mapping of "raw BACnet name -> meaning". For example, you might conclude SF_Stat means "Supply Fan Status" (a binary sensor on the supply fan), Zone_Temp is "Zone Temperature (sensor)", and Zone_Temp_SP is "Zone Temperature Setpoint". This sets the stage for creating standardized aliases and tags.

## Creating Standardized Aliases and Tags (Project Haystack)

Now that I've interpreted the raw labels, the next step is to express them in a consistent, standardized form. This involves two things: assigning a human-friendly name (alias), and annotating each point with semantic tags that software (like SkySpark or analytics tools) can use to understand the data. A well-established approach to this is using Project Haystack tagging, which SkySpark natively supports.

**Why Tagging?** In a traditional BAS, you might have a naming convention but no additional metadata – the name is the only clue to a point's meaning. Semantic tagging instead attaches multiple descriptors. For example, rather than relying on "AHU1_SAT" string, I attach tags: `{ "point": true, "temp": true, "sensor": true, "air": true, "discharge": true, "unit": "°F" }`. This set of tags unambiguously tells us this point is a sensor measuring air temperature in a discharge (supply) air stream, and its unit is degrees Fahrenheit. Another point like a cooling setpoint might have `{ "point": true, "temp": true, "sp": true, "cooling": true, "unit": "°F" }`. Tags make the data self-describing and machine-interpretable – a critical feature for scaling analytics and integrations.

**Project Haystack Tags:** Project Haystack provides a dictionary of standardized tag names and models for building data. Some key tag concepts I'll use are:

- **Equipment markers:** e.g., ahu, vav, chiller, boiler, etc., which you would put on the equipment record to identify its type. Equipment records also carry the equip marker tag.
- **Point markers:** Every point gets a point marker (SkySpark usually adds this by default for point records). Further, Haystack uses tags to distinguish point roles:
- A sensor point (measuring a value) often gets a tag like sensor (Haystack 4 uses sensor as a marker on many measured points). In Haystack 3, one would indicate sensor vs command implicitly by the presence of other tags (e.g., a setpoint had sp tag, a command might be indicated by cmd tag).
- Setpoints use the sp (setpoint) marker.
- Commands (actuator outputs) might use cmd marker (Haystack 4 formalizes cmd as well).
- Status or feedback points might have status in their name but effectively they are sensors (they sense the state of equipment). One can tag them with sensor and perhaps another tag like run (for a fan run status) or alarm (if it's an alarm status).
- **Kind and unit:** Use kind:"Number" or "Bool" to indicate numeric vs boolean, and unit:"°F" or "%" etc. for units. These are important for analytics and correct UI display. (SkySpark will often fill these in from BACnet automatically, but you can adjust if needed.)
- **Quantities and substances:** Tags like temp, pressure, humidity describe what is being measured. Tags like air, water describe the medium. Combined with context tags like zone, discharge, return, outside (air) or entering, leaving (water flows), they pinpoint what the measurement is. For example, temp + air + discharge together mean discharge-air temperature.
- **Ref tags:** equipRef links a point to its equipment, and siteRef links to the site (building). These are relational tags that form the hierarchy in a Haystack model.
- **Navigation/Display tags:** navName is used by SkySpark for tree navigation naming (often set to the raw name or alias), and dis is a human-readable display name. I will focus on setting dis for the alias.

Using these tags, you build a rich description. For instance, after parsing, you decided SF_Stat means "Supply Fan Status" on AHU-1. In Haystack tagging, on that point's record you would mark: fan (because it's about a fan), supply (because AHU supply fan), sensor (it's sensing status), maybe run (Haystack has a run marker for run/stop status), and bool kind with no unit (since it's on/off). You might give it a dis of "Supply Fan Status". Similarly, Zone_Temp_SP on a VAV would get tags: zone, temp, sp, maybe occupied (if it's the occupied cooling setpoint vs heating setpoint, there are ways to differentiate, but let's assume one setpoint), and unit:"°F". Its dis could be "Zone Temperature Setpoint".

**Consistent Aliases (Display Names):** When creating aliases, decide on a format that is clear and consistent. Many choose a [EquipmentName] [Point Description] format, e.g., "AHU-1 Supply Air Temperature". In SkySpark, you can automate this via a disMacro – for example, setting disMacro:"$equipRef $navName" will produce a display name that prepends the equipment name to the point's name. In the earlier code snippet, they used exactly that to name points like "Misc RoomTemp" etc. You can then manually tweak dis if needed (perhaps to expand abbreviations). The goal is that anyone reading the alias understands what and where that point is. Use full words in aliases (e.g., "Temperature" instead of "Temp", "Setpoint" instead of "SP") for clarity.

**Standards and Consistency:** Adopting an existing standard like Project Haystack means you benefit from industry consensus. For example, tagging a point with temp and sensor and zone will mean the same in any Haystack-compatible software (and to any experienced engineer): a zone temperature sensor. If later you use analytics rules or connect another system (like an energy management system or a fault detection tool), these tags let that software automatically identify points of interest. SkySpark's analytics engine, for instance, can use tags to apply rules to all similar points (find all air handling units and check if supply temp minus return temp is within range, etc.). In short, proper tagging "gives your data meaning" that both SkySpark and external applications can leverage.

**Project Haystack vs. Other Frameworks:** While Haystack is already integrated with SkySpark, it's worth noting the existence of Brick Schema as an alternative. Brick is an ontology (often represented in RDF/OWL) that defines equipment and points in a graph with relationships. In Brick, one would classify points into classes like Brick:Supply_Air_Temperature_Sensor. The end goal is similar – standardized metadata – though the implementation differs. If you were to use Brick, the process of parsing names would be the same, but instead of assigning Haystack tags, you'd map each point to a Brick class and link it to Brick equipment classes. There are also emerging standards (ASHRAE Project 223P) aiming to incorporate semantic tags directly into protocols like BACnet. The good news is these efforts are converging: Haystack 4 and Brick are being aligned to some extent. For my project, sticking with Haystack (since SkySpark and CxAlloy likely align well with it) is pragmatic. Just be aware that the concepts here (aliasing and tagging) are applicable universally – whether tags or classes, the idea is to attach standardized semantic descriptors to each point.

## Example: Before and After Tagging

To solidify, let me walk through one example. Suppose I have a BACnet point with:
- Object Name: VAV1_ZN-T (on device VAV1)
- Description: Zone Temperature
- Units: °C (Celsius)
- Object Type: Analog Input

Before tagging, when this comes into SkySpark, it might create a record with navName:"VAV1_ZN-T", unit:"°C", kind:"Number", and a connector reference. It's hard to tell what that is just from VAV1_ZN-T.

After my normalization, I would have an equipment record for VAV1 tagged with vav (and equip). The point record I enrich as:
- dis: "VAV1 Zone Temperature" (clear alias)
- Tags: point, sensor, temp, zone (plus unit:"°C", kind:"Number", equipRef:@id-of-VAV1)

Now anyone can see it's the zone temperature sensor. And if using Haystack conventions, you might also tag the VAV's zone (if zones are modeled) or other context, but that's optional for this integration.

The overarching principle is: each point gets labeled with what it is (temperature, setpoint, etc.), where it is (zone, discharge, etc.), and how it functions (sensor or command, etc.). With that in place, both humans and machines can reliably identify the point.

## Leveraging Equipment Metadata and Vendor Information

In addition to the point names themselves, I have other metadata about the equipment and systems that can greatly assist in mapping:

**Equipment Type Inference:** I touched on this above, but it's worth emphasizing as its own step. If your data source (file name or a field in the trio file) indicates the equipment type or at least the equipment name, use that aggressively. Create a mapping from equipment name to type. For example, if an equipment is named "AHU-1" or the file is AHU1.trio, I know it's an Air Handling Unit. If something is "RTU" (rooftop unit) or "FCU" (fan coil unit), or includes "VAV" in the name, those are clues. Tagging the equipment record appropriately (with ahu, rtu, vav, etc.) not only is good practice for the data model, but it lets you apply type-specific point mapping. You can maintain a template of expected points per equipment type. For instance, for any ahu, you expect maybe SAT, RAT, SupplyFan status/command, Cooling coil valve, etc. For a vav, you expect zone temp, damper position, perhaps reheat coil etc. If the equipment name or model doesn't directly tell you, consider the manufacturer model number: a model might be known to correspond to a type of equipment (e.g., model contains "VMA" which in Johnson Controls jargon is a VAV Modular Assembly for VAVs).

**Manufacturer's Point List:** If you know the manufacturer and model of the controller or equipment, try to find documentation. Many vendors publish integration manuals for their equipment with standard BACnet point lists. For example, a packaged RTU (rooftop unit) from Carrier or Trane might have a fixed list of BACnet points (with standard names or at least standard descriptions). If you find that document, you essentially have a direct translation of cryptic names. Even if the integrator renamed points, the descriptions often remain similar. Matching your points against such a list can validate your interpretation. For instance, if Trane RTU model X has a point called "SpaceTemp" for zone temperature and you see a point "ZN-T" in the BACnet export, it's likely the same thing labeled differently. Vendor documentation can also clarify units and ranges (e.g., a valve command might be 0-100%). Leverage any standard tag libraries from manufacturers if available. Some systems like Siemens Desigo or Johnson Controls Facility Explorer come with naming conventions; knowing those can decode their abbreviations.

**Vendor-Provided Descriptions and Tags:** In some modern systems, vendors might already include tagging (Project Haystack tags, Brick metadata, or BACnet standardized tags) in the controller's configuration. For instance, if the site was engineered with semantic tagging in mind (using something like Siemens' Haystack support or JCI's tagging), you might find the trio files already have tags or standardized names. Check if the trio file has any existing tags beyond just point and equipRef. If, say, you find tags like ahu or temp already present, that means some of the work was done by the source system. Use and refine that rather than reinventing it. Even if no tags, sometimes the "Description" field in BACnet was filled in with a nice description. I mentioned this, but to reiterate: it's common for the controls technician to label the point nicely in the front-end software while the BACnet object name remains short. If you have that, use the description as the dis (alias) directly, and then just add appropriate semantic tags to it. For example, Object Name ZN-T1, Description "Conf Room 101 Temp Sensor" – you'd set alias "Conference Room 101 Temperature" perhaps and tags temp, zone, sensor. The description gave you context (room 101, zone sensor).

**Consistency with Vendor Conventions:** If all equipment of a type from a vendor uses the same naming convention, you can generalize. For instance, Johnson Controls VAV controllers (if using their standard Application) might name the zone temperature as "ZN-T" consistently. So once you decode one JCI VAV, you can apply that to all others on the project. The earlier forum discussion noted that some OEMs like Johnson have pretty consistent point naming for their applications (e.g., DA-T for discharge air temp, RA-T for return air temp in their AHU application, etc.), whereas others leave more to the installer's preference. If you detect such a pattern, treat it as a mini-dictionary for that subset of points. It can speed up tagging immensely.

**Using Model and Serial Data:** CxAlloy likely tracks equipment details such as manufacturer, model, serial number, etc. Ensuring that the equipment records in SkySpark have those (they can be added as tags or as additional properties) could help in mapping and in the integration. For example, if you have a custom mapping script, it could choose a different tagging approach if it sees manufacturer: "Johnson Controls" vs manufacturer: "Siemens". Or at least, later on in CxAlloy, you might want to verify that the right model's data is mapped. It's a good practice to propagate such metadata into SkySpark so all relevant info is in one place.

**Cross-Validation with Live Data:** Once you've tagged and aliased points using all the above info, do a reality check by looking at live data trends (if the system is running). Does the "Zone Temperature" you identified actually track a sensible room temperature? Does the "Valve Command" go from 0 to 100% when heating is on? If something labeled "Fan Status" never turns to "On", maybe it's actually a command that wasn't enabled, or vice versa. Tuning your mappings with real data prevents mistakes (like tagging a point as a sensor when it's really a command that was inactive, etc.). This is part of commissioning as well – verify that points are reporting as expected. In SkySpark, you can bring up a quick chart of all points on an equipment to see their values over a day and see if any behavior looks off for what you tagged them as.

In short, the more context and metadata I use, the more accurate my point normalization will be. Equipment type narrows the possibilities, vendor info can provide a direct name-to-meaning map, and descriptions give natural language hints. Combining these with my naming pattern analysis yields a robust mapping.

## Automation with Pattern Recognition and Machine Learning

For large systems, manually mapping hundreds or thousands of points is error-prone and time-consuming. That's why I look to automation techniques. I've already been using pattern recognition (via dictionaries and rules). I can take that further, and even consider machine learning to improve the process:

**Rule-Based Tagging Scripts:** This is often the most practical first step (and sometimes all you need). You define a set of if-then rules or regular expression patterns to recognize common strings in point names and assign tags accordingly. I saw an example of this using SkySpark's Axon code: checking if the name contains certain keywords like "temp", "air", "zone", and adding those tags. A rule might be as simple as "if name contains "_SP" or ends with SP, add sp tag (mark it as a setpoint)". Another: "if name contains Valve or abbreviation VL and units are %, tag as a valve position command". These rules can cascade and get quite specific. Using regular expressions, you can capture patterns like ^AHU\d+_SF_? to identify supply fan points on AHUs. The idea is to encode your knowledge from the parsing stage into a script that can apply it in bulk. SkySpark's scripting and find functions allow complex text matching, so you could script: find all points whose raw name matches certain regex and apply a set of tags to them.

**Community Dictionaries:** As discussed in the Project Haystack community, one approach is to maintain CSV or JSON dictionaries of common point name fragments to tag mappings. For example, a CSV might say: "SA Temp","temp, sensor, air, discharge" meaning if a point name equals or contains "SA Temp", assign those tags. You could even have vendor-specific dictionaries: one for Johnson Controls naming (listing all their acronyms), one for Trane, one for Siemens, etc. Then your automation script could load the relevant dictionary based on the controller manufacturer tag. This approach was suggested as a collaborative effort – to build an "ever-growing lookup library" of standard point names to tags. While a universal library isn't yet fully established, you can build your own for your projects and incrementally expand it. Think of it as a custom translation table that gets smarter over time.

**Machine Learning Approaches:** In recent years, researchers and some advanced solution providers have been experimenting with ML to automatically classify and tag points. Here are some insights and possibilities:

- **Text Classification:** You can treat the point naming (plus available metadata) as an input to a classifier. The output would be a classification into a point type or a set of tags. For instance, you could train a model on a bunch of points labeled as "Zone Temperature Sensor", "Supply Fan Command", etc. as categories. Features fed to the model would include character n-grams from the point name, whether the name contains digits (floor or room numbers), the unit, object type, maybe the equipment type, etc. Classical algorithms like Support Vector Machines or Logistic Regression have been applied with some success. These models essentially learn which keywords or patterns correlate with which tag outputs.

- **Natural Language Processing (NLP) techniques:** Even though point names are not exactly natural language, techniques like Latent Semantic Indexing or clustering can be used to find similarities between naming conventions. For example, you might vectorize point name strings into some numerical form and use clustering to group similar points (which might correspond to similar functions).

- **Neural Networks:** Some projects have tried LSTM or transformer-based models to parse point names (treating it almost like a translation: raw name -> standardized classification). This can capture more complex patterns and even correct for typos or unconventional abbreviations, given enough training data.

- **Time-Series Patterns:** A unique angle in building data is using the behavior of the data to aid classification. Research has shown you can distinguish sensors from setpoints by looking at their time-series: sensors typically fluctuate around a setpoint, while setpoints change in steps and remain flat until adjusted. Similarly, a command vs a status might be inferred if you see one point toggles and another follows it, etc. Some advanced algorithms might incorporate a short historical sample of data as features (e.g., variance, range, periodicity) for classification. In practice, this is harder to do in an automated way because it requires data collection, but it can significantly improve accuracy when the names alone are ambiguous.

- **Existing Tools and Research:** The field has seen prototypes like a DOE-funded tool by UTRC (United Technologies Research Center) that used machine learning to auto-tag BAS points. Academic work from places like UC Berkeley and University of Virginia investigated methods to infer metadata from point names and trends. For example, one project called "Deconstructing HVAC Point Names" used a combination of dictionary methods and probabilistic graphical models to interpret point names. Another looked at identifying functional relationships between sensor streams. These projects report high accuracy in test cases, though they are not turnkey products yet. A PhD student involved noted that while not production-ready, they show promising avenues for automated metadata extraction.

- **Continuous Learning:** One of the most powerful aspects of ML is the ability to improve with more data. If you integrate many buildings, you can accumulate a training dataset. Your workflow could include a feedback loop: when you manually correct or tag a point that automation didn't get right, feed that back into the algorithm (update your dictionary or retrain the model). Over time, the tool becomes more capable. In effect, the system builds an "ever expanding dictionary" and model that learns from each new project. This is akin to how spam filters get better by learning from user corrections.

**Hybrid Approach:** It's not an either/or between rules and ML. The state-of-the-art typically uses a hybrid approach:
- Use rule-based parsing for what it's good at (the low-hanging fruit where simple patterns cover many points).
- Use ML-based suggestions for the rest. For instance, an ML model might flag that a point likely represents a temperature sensor vs a valve, but you might still apply a rule to decide if it's discharge or return based on name. Or vice versa: use ML to catch weird abbreviations that your rules missed.
- Have a human verification step where the automated tags are reviewed (either in bulk via a report or individually for critical points). This aligns with commissioning best practices – you always want to verify that sensors and controls are correctly identified and mapped.

For my project, consider starting with a rule-based script (since I have a defined set of equipment and presumably some consistency within it). If I find the naming is very inconsistent or complex, and if I have time for a more advanced solution, I could incorporate an ML classifier. Python has libraries like scikit-learn which could be used alongside SkySpark (via its REST API or via data export/import) to do a one-time classification of points based on training data. However, often a well-crafted set of rules with a good dictionary can get you 90% of the way quickly.

In summary, automation can significantly speed up the tagging process. Pattern recognition through heuristics handles the straightforward mappings quickly, and machine learning can tackle the edge cases and improve accuracy by learning from data. By applying these, I'll minimize manual effort while maintaining a high-quality mapping.

## SkySpark-Specific Tools for Mapping and Normalization

Working within SkySpark provides some powerful features to assist with this normalization process. Here are SkySpark-centric techniques and tools to use:

**1. Connector Setup and Point Import:** Ensure you've configured the BACnet connector in SkySpark. This involves defining the BACnet network (IP address, BBMD settings if needed, etc.) and then discovering devices. SkySpark's interface (in the Connectors app) will list discovered BACnet devices and allow you to scan their points. This is analogous to FIN's DB Builder discovery process. You can add each device as a SkySpark "device" record (with device tag, typically) and then import the points. If you already have the points via trio files, you might instead import those files. But it's good to know you can also do direct discovery for any missing points or to verify connectivity.

**2. Using Trio/CSV Import:** SkySpark can import data in trio format directly. If you have those trio files per equipment, you can use the SkySpark shell or Axon functions to import them. For example, Axon has readAll(fileRead("equipment1.trio")) to read records from a trio file. Similarly, CSV can be imported via ioReadCsv() as shown in Adam's example. The imported grid of points can then be iterated over to set tags. So, one strategy is: keep your source trio as a baseline, then write an Axon script that reads it, applies transformations (adding tags/alias), and commits the changes.

**3. Axon Scripting – Bulk Tagging Logic:** Axon (SkySpark's scripting language) is extremely handy for this kind of bulk data manipulation. You can write scripts to find and tag points based on name patterns. I saw a snippet earlier where they did `if (value.contains("temp")) marker()` to add a temp tag. You can get much more sophisticated:
- Axon supports regex via functions like find on strings or using the fin library (if installed).
- You can loop through all points of an equipment or all points in the project and use conditional logic to assign tags.
- You can even create a SkySpark function that takes a raw name and returns a dict of tags to add, encapsulating your rules, then map that over all points.
- The snippet by Adam also shows setting the kind and unit based on name heuristics (e.g., if name contains "status" then Bool and a cmd tag).
- Because SkySpark's database is tag-based, you can use set operations: e.g., point.set(temp, marker()) to add a tag. Or construct a whole point record in memory with all tags and then use commit().

For example, an Axon approach might look like:

```
readAll(point and equipRef == @ahu1) 
 -> forEach p {
 nameLower := p->navName.toLower
 if(nameLower.contains("temp")) { p->temp = marker(); p->unit = "°F" }
 if(nameLower.contains("zone")) { p->zone = marker() }
 if(nameLower.contains("sp")) { p->sp = marker() }
 if(nameLower.contains("status") or nameLower.contains("sts")) {
 p->cmd = marker(); p->kind = "Bool"
 } else {
 p->sensor = marker(); 
 if(p->kind == null) p->kind = "Number"
 }
 // ... other rules ...
 p->dis := formatDisplayName(p) // maybe a function to nicely format name
 commit(p)
 }
```

The above is pseudo-code, but it illustrates using string contains to set tags. The real code may differ in syntax. The key point: Axon lets you script the transformation inside SkySpark's environment, which means you don't have to export, process externally, and re-import – you can do it in place and leverage the live data for checks if needed.

**4. Verification Tools in SkySpark:** After tagging, you can use SkySpark's UI to verify. The Equip view will list all points under an equipment, showing their navName and dis and maybe some tags. This is a quick sanity check. There's also a Dictionary or Model view in newer SkySpark versions (especially with Haystack 4 defs enabled) that can show how many points have a given tag, etc. You can run Axon queries in SkySpark's Tools or Rules app to verify things: e.g., `count(readAll(point and temp and sensor))` to see how many temp sensors you have, or `readAll(point and not (temp or pressure or humidity or power or binary))` to find points that have no primary measurement tag (meaning you might have missed tagging them). Using such queries, you ensure no point is left behind untagged or mis-tagged.

**5. SkySpark Rules for Data Quality:** You could even write some SkySpark rules that flag anomalies in tagging. For instance, if an equipment has an ahu marker, you expect at least one discharge air temp on it; if not, maybe you missed mapping the SAT. Or if a VAV has a reheat coil (reheat tag on equip), you expect a valve command point. These can be formalized as rules that produce warnings if something looks off. While not explicitly asked, it's a nice extension of using SkySpark's analytics to validate your semantic model.

**6. Using the Haystack REST API for Bulk Operations:** SkySpark's API (which implements the Haystack standard) can also be used for this kind of work. For example, you could export all points to a JSON or Zinc format, run an external script (Python, etc.) to apply naming rules (perhaps using a more comfortable text processing environment or ML model), and then PUT the updated records back via the API. This is an alternative if you find Axon limiting or prefer external tooling. Since SkySpark is a native Haystack server, any tool that can speak Haystack can read/write to it securely. This could be how you interface a machine learning model: SkySpark provides raw data (point names, units, etc) via REST; your Python ML code returns a set of tags for each; then you update SkySpark records accordingly.

**7. SkySpark Extensibility:** There might be existing SkySpark extensions (on StackHub or elsewhere) that attempt to do auto-tagging. It's worth searching the SkyFoundry StackHub for terms like "auto tag" or "BACnet tagging". Even if none are publicly available, the community forum is a resource – others have tackled similar tasks, as evidenced by the code snippet I used from the Project Haystack forum and discussions around it.

**8. Writing Back to BACnet?** This may be tangential, but note that none of this changes anything on the BACnet side; it's all within SkySpark's layer. However, if you ever have the influence on the BAS configuration phase, advocating for better naming upfront or even standardized tags (some controllers can expose Haystack tags or at least standardized object names) will reduce effort. BACnet is evolving to include semantic tags natively (addendum to BACnet standard for tagging). The industry vision is that one day devices might self-report tags, making this mapping largely automatic. Until then, my approach in SkySpark is effectively creating that semantic layer on top of BACnet.

**9. History and Trend Setup:** Since a goal is also viewing trend data, ensure the SkySpark points have his markers (history enabled). In the code snippet earlier, they explicitly added his to each point record to tell SkySpark to store history. You should check that in your SkySpark project configuration (usually, by default, imported points might not have history enabled unless specified). You can enable it in bulk with a simple Axon: `readAll(point and not his) -> each{ it->his = marker(); commit(it) }`. Also verify history settings like how often to poll or COV subscription. Once history is flowing, SkySpark's Historian app will let you graph data, or you can query it for certain time ranges when running functional tests.

**10. Utilizing SkySpark's Analytics:** With tags in place, you can also use SkySpark's rule engine to automate some functional tests. For example, you might write an Axon rule that uses tags to find a zone temperature sensor and its setpoint and check their relationship during a test (like did the temperature reach the setpoint within a certain time after a command). The tags make such rules generic; the same rule could apply to all VAVs because it can do `temp and zone and sensor` to get the sensor and `temp and zone and sp` to get the setpoint. This ties into my goal of automated functional testing – I can embed test logic in SkySpark or at least data monitoring logic, and then feed results to CxAlloy as needed.

In essence, SkySpark provides the sandbox to implement all the mapping and tagging. Its connector brings in the data, and its combination of UI, Axon scripting, and API give multiple avenues to perform the normalization. By leveraging these, I avoid doing everything manually point-by-point and instead script the bulk of the work, while keeping flexibility for fine-tuning. The end result in SkySpark will be a fully tagged equipment and point structure, readily navigable and queryable.

## Workflow for Normalizing Points and Mapping to CxAlloy

Bringing it all together, here is a structured workflow to achieve the normalization and aliasing of BACnet points and integrate them with CxAlloy's equipment definitions:

**1. Inventory and Export of Raw Data:** Start by collecting all point data from the BMS. In my case, this is given as one trio file per equipment. Ensure I also gather context like the equipment list (with names/types), and any known info such as manufacturer, model, and BACnet device IDs. If trio files contain some tags (even if just navName and equip references), load them into a staging SkySpark project or a spreadsheet for review. The idea is to have a complete list of points grouped by equipment.

**2. Create Equipment Entities in SkySpark:** Before diving into points, create records for each piece of equipment in SkySpark. Assign each an equip marker and an appropriate equipment type tag (ahu, vav, chiller, etc.), and link them to the site or building record via siteRef. Also include any known attributes like manufacturer, model, as tags or as marker tags (you can use geoCity, vendor, etc., or custom tags). This establishes the structure. For example, add "AHU-1" with tags `{equip, ahu, dis:"AHU-1", floor:"1", area:"Lobby"}` (and siteRef to building). Do this for all equipment. These can be imported via trio as well.

**3. Parse and Plan (Offline or Scripted):** Using the methods from the parsing section, interpret each point name and jot down the intended meaning. At this stage, it might help to create a mapping table in a spreadsheet or text document with columns like: "Equipment, Raw Point Name, Interpreted Meaning, Planned Alias, Tags to Apply". This is essentially a design document for my tagging. For example:

| Equip | Raw Name | Interpreted Meaning | Alias (dis) | Tags (to add) |
|-------|----------|---------------------|-------------|---------------|
| AHU-1 | SAT | Supply air temperature sensor | AHU-1 Supply Air Temp | temp, sensor, air, discharge, unit:°F |
| AHU-1 | SAT_SP | Supply air temperature setpoint | AHU-1 SAT Setpoint | temp, sp, air, discharge, unit:°F |
| AHU-1 | SF | Supply fan command | AHU-1 Supply Fan Command | fan, cmd, supply, kind:Bool |
| AHU-1 | SF_Status | Supply fan status (proof) | AHU-1 Supply Fan Status | fan, sensor, supply, run, kind:Bool |
| AHU-1 | Mixed_Air_T | Mixed air temperature sensor | AHU-1 Mixed Air Temp | temp, sensor, air, mixed, unit:°F |

Do this for each point. This can be time-consuming but is crucial for correctness. If I have many repetitive points, do a few for one equipment then generalize patterns for others. In the above table, if "AHU-2" has similar points, I know to apply the same tags there.

**4. Build or Refine the Tagging Rules:** Based on the mapping plan, decide which parts I can automate. For instance, if every point that ends with _SP is a setpoint with certain tags, implement a rule for that. If "SAT" appears, it's an air temp sensor on discharge – implement a rule for "SAT" specifically (perhaps guarded by equip type = AHU to avoid mis-tagging something else that coincidentally has SAT). If some points don't fit a simple pattern (e.g., "EF" could mean Exhaust Fan or something else depending on context), I might handle those individually or with more complex logic (like check if equip has an exhaust fan).

If using Axon in SkySpark, I might write a function tagPoint(p) that checks p->navName against a series of conditions and then does p->set(tag, value) accordingly. If using an external script or just doing it by hand in trio, outline those conditions so I apply them consistently. Essentially, I'm encoding the mapping table into logic.

**5. Apply Tags and Aliases in Bulk:** Execute the tagging. In SkySpark, this could be running my Axon script over all points. If my points aren't actually in SkySpark yet (only in trio files), I have two options:
- **Option A:** Import the raw trio files into SkySpark first (with minimal tags), then run the Axon script to add tags in place. This way I leverage SkySpark's environment to do the processing and can verify as I go.
- **Option B:** Write a script to transform the trio files themselves (e.g., using a Python script that reads the trio as text, adds lines for new tags, etc.), then import the modified trio into SkySpark.

Option A is often easier because SkySpark has the tools to manipulate tags in-memory. For example, I can do:

```
readAll(point and equipRef == @ahu_1) |> tagAhuPoints
readAll(point and equipRef -> equip and ahu) |> tagAhuPoints
readAll(point and equipRef -> equip and vav) |> tagVavPoints
```

where tagAhuPoints and tagVavPoints are functions I wrote for each equipment type's rules.

When this step is done, each point record in SkySpark should now have a full set of tags and a nice dis name. If I were editing trio files externally, at this point I would import them back into SkySpark to create/update records.

**6. Quality Check in SkySpark:** Use the SkySpark UI and queries to verify the tagging. For each equipment, open it in the SkySpark App (the Equip view) and see that points have the expected aliases. Check a few point details to ensure no obvious mistakes (e.g., two different points accidentally got the same alias, or a numeric sensor was mistakenly marked as Bool, etc.). If I find issues, correct them either by adjusting the tags manually or refining my rules and rerunning (I can safely run the tagging script multiple times; if it's written idempotently, it will just set the same tags again or fill in new ones). This is also a good time to compare with any external documentation or the BMS front-end: do the names I came up with make sense to others who know the system? If a controls engineer is available, a quick review with them can validate that my "Mixed Air Temp" is indeed what they know as "MAT", etc.

**7. Set up History and Validate Data Flow:** Enable history (his tag) on all points where trend data is needed (likely most sensor points and maybe setpoints; perhaps commands if I want to log those too). Ensure SkySpark is actually pulling values from the BACnet network. I might see real-time values in the SkySpark "Points" view or by looking at the point records in the Watch view. This serves as a final confirmation that each SkySpark point is correctly bound to the BACnet point (sometimes a mis-configured device ID or object ID could lead to no data – better to catch that now). Once values are coming in, check a couple of trends over a day to see if the behavior matches expectations (temperature rises and falls reasonably, binary statuses toggle when equipment runs, etc.). This not only validates the data but also serves as baseline data for commissioning tests.

**8. Integrate with CxAlloy – Equipment Mapping:** Now that SkySpark has clean data, turn to CxAlloy. CxAlloy likely has a concept of equipment and possibly test forms or checklists expecting certain readings. The integration might happen via API or data import. First, ensure that the equipment naming in CxAlloy matches the equipment naming in SkySpark. For example, if CxAlloy's equipment list was imported from a design schedule, the names might be "AHU-1", "AHU-2" etc., which is great if they match SkySpark. If not, create a mapping (maybe as simple as a lookup table) to reconcile any differences. Consistent equipment identifiers are crucial to avoid confusion – I don't want to mix up AHU-1 and AHU-2 data.

If CxAlloy allows custom fields or attributes on equipment, I could also store the SkySpark id or some link there, but that might not be necessary. The key is a common name or code.

**9. Integrate with CxAlloy – Point Mapping:** Determine how CxAlloy will consume point data:
- Some commissioning platforms allow importing trend logs or live values to attach to functional test results.
- Others might just need you to manually check values, but perhaps I want to automate that.

With the SkySpark data normalized, I can now reference points by their aliases or tags. For example, suppose a functional test step in CxAlloy is "Verify supply air temperature rises by 5°F when heating valve opens." Through the integration, I could have SkySpark automatically find "AHU-1 Supply Air Temp" point (since it's tagged, or by alias) and "AHU-1 Heating Valve Command" point, then pull their trend data during the test interval and calculate the change. The results (e.g., trend graph or pass/fail) could be pushed to CxAlloy via its API.

On a simpler level, I might just want CxAlloy to display current values or recent trend snapshots for each relevant point during testing. In that case, my integration could do an API call to SkySpark for the current value of "AHU-1 Supply Air Temp" and display it in CxAlloy's interface.

Regardless of use case, I need a mapping of what each point is called in CxAlloy's context versus SkySpark. The ideal scenario is they are identical. If I have the freedom to define point names in CxAlloy, use the aliases I set as those names. For instance, in a CxAlloy checklist for AHU-1, I might have a field "Supply Air Temperature" – make sure that corresponds exactly to the SkySpark alias "AHU-1 Supply Air Temp" (or at least "Supply Air Temp" under AHU-1). Then I know that when querying SkySpark, I query by that name. If CxAlloy organizes points by equipment and a standard nomenclature, stick to that nomenclature when creating aliases.

**10. Use the CxAlloy API for Automation:** CxAlloy provides an API that I can use to programmatically send and retrieve data. With my points normalized:
- I could write a script (in Python, Node, or even Axon can make HTTP calls if needed) that for each piece of equipment, pulls the latest values or a block of historical data from SkySpark (SkySpark has its own REST API to get point history in Haystack format) and then sends that data to CxAlloy's API to, say, update a field or attach a file (like a CSV or image of a trend graph) to a test issue.
- If doing automated functional testing, I might orchestrate a sequence: use SkySpark to command an actuator (SkySpark can write to BACnet points too, if allowed), wait, measure the response via trend, then post the result to CxAlloy (e.g., mark the test step as passed/failed with evidence data).
- Ensure security and rate limiting are considered – I might be pulling lots of data. The good news is, with tagging, I can request exactly what I need. For instance, I can query SkySpark for all points with discharge air temp tag and get their current values, rather than hardcoding a list of point IDs.

**11. Testing and Calibration of Integration:** Do a pilot test with one or two pieces of equipment. For AHU-1, try to fetch a few points' data from SkySpark and push to CxAlloy or verify it matches what CxAlloy expects. This will flush out any remaining mapping mismatches. Maybe I discover CxAlloy expects a field named "Supply Air Temp (°F)" – I might then adjust my alias to include units or adjust how I present it to CxAlloy. It's easier to adjust at this stage now that my core data is tagged; changes like renaming an alias in SkySpark are trivial and can propagate via the integration.

**12. Documentation and Future Use:** Document the rules and mappings I created. Keep a record of my acronyms dictionary, any custom handling I had to do, and where I stored the mapping logic (Axon functions, scripts, etc.). This will be invaluable for maintaining the integration (e.g., if a new point is added later by the controls team, I'll know how to tag it) and for replicating the process on future projects or other sites. As a final step, I might export the fully tagged data set (SkySpark can export a project to trio or Zinc) as documentation of the BAS – essentially I've created a semantically rich points list that others can read.

By following this workflow, I transform the initial cryptic data into a structured, standardized form. The SkySpark side now serves as a semantic database of the BAS, and CxAlloy can pull from it or receive updates with confidence that "Zone Temperature" really means the zone temperature. The commissioning process becomes more automated: tests can reference points by name, trend data can be correlated automatically, and any issues found can be pinpointed to the correct sensors/actuators without guesswork. Moreover, this normalization process has a one-time cost but yields long-term benefits – not only for CxAlloy integration but for any other analytics or building management tasks down the line.

## Conclusion

Integrating SkySpark with CxAlloy for automated testing and data analysis is greatly facilitated by a robust point normalization strategy. Through the steps I've outlined – from decoding BACnet point names to applying Project Haystack semantic tags – I achieve a common language between the BMS data and the commissioning tools. In SkySpark, the formerly cryptic points become self-describing records enriched with standardized tags and human-readable aliases. This semantic layer means I no longer have to manually link "Point A" to "meaning X" – the metadata carries that meaning. As a result, SkySpark can present and analyze data in context, and CxAlloy can consume the information it needs without custom per-point configuration.

By leveraging equipment context and vendor documentation, I ensure the aliases and tags are accurate and insightful (e.g., distinguishing a supply fan command from a status). Using pattern recognition and even machine learning techniques accelerates the tagging process and reduces errors, especially in large systems, moving me closer to the ideal of fully automated point mapping. SkySpark's flexible environment allows me to script and iterate on the tagging workflow, and its native use of Haystack tagging means I'm aligned with industry standards from the get-go. In effect, I've enhanced the BACnet integration with an additional metadata layer – "taking the best of both worlds; enhancing the standard protocol (BACnet) with an open metadata standard (Haystack)" to make the devices self-describing.

For the final integration, because points now have consistent names and tags, creating a link to CxAlloy is relatively straightforward. Each equipment's points in SkySpark correspond clearly to CxAlloy's expected data points. Whether via direct API calls or data exports, CxAlloy can receive trend logs or real-time values labeled in a way that matches the commissioning test definitions. This eliminates a great deal of manual cross-referencing and ensures that automated tests in CxAlloy are pulling the right data from SkySpark every time.

In summary, the workflow provides a repeatable blueprint for turning raw BAS point data into a structured dataset ready for analytics and integration. By normalizing and aliasing BACnet points using tags and standard names, I unlock interoperability between systems: SkySpark's analytics, CxAlloy's commissioning management, and any future applications can all refer to the same points with the same terminology. The upfront effort in data modeling pays off in a seamless integration where engineering intent (e.g., "Supply Air Temperature") is directly connected to live data, enabling smarter commissioning and building operation. With this approach in place, I can focus on higher-level tasks (like optimizing test procedures or analyzing results), rather than wrangling point names, thus streamlining the overall project workflow.

**References:**
- Step by step from Bacnet names to haystack tagging – Project Haystack: https://project-haystack.org/forum/topic/638
- CxAlloy TQ API: https://api.cxalloy.com/